# -*- coding: utf-8 -*-
"""Pre-process Tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A92S3vxkDaDpdbzZYb8eLVtlRgj64_mC
"""

import numpy as np
import pandas as pd
import nltk
nltk.download('twitter_samples')
nltk.download('punkt')
from nltk.corpus import twitter_samples 
import re

def remove_nonascii(dataset):
  processed_dataset = []
  for i, tweet in enumerate(dataset):
      encoded_tweet = tweet.encode('ascii', 'ignore')
      processed_dataset.append(encoded_tweet.decode())
  return processed_dataset

def remove_hashtags(dataset):
    processed_dataset = []
    for i, tweet in enumerate(dataset):
        processed_dataset.append(re.sub(r'#[A-Za-z0-9_]+', '', tweet))
    return processed_dataset

def remove_hyperlinks(dataset):
    processed_dataset = []
    for i, tweet in enumerate(dataset):
        processed_dataset.append(re.sub(r'(https?:\/\/)?([\da-z\.-]+)\.([a-z\.]{2,6})([\/\w \.-]*)', r'', tweet))
    return processed_dataset

from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize

def remove_stopwords(dataset):
    processed_dataset = []
    stop_words = set(stopwords.words('english'))
    for i, tweet in enumerate(dataset):
        tweet_tokens = word_tokenize(tweet)
        filtered_tweet_tokens = [w for w in tweet_tokens if not w.lower() in stop_words]
        processed_dataset.append(' '.join(filtered_tweet_tokens))
    return processed_dataset

def remove_punctuations(dataset):
    processed_dataset = []
    punc = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''
    for i, tweet in enumerate(dataset):
        for element in tweet:
            if element in punc:
                tweet = tweet.replace(element, '')
        processed_dataset.append(tweet)
    return processed_dataset

from nltk.stem import PorterStemmer
porter = PorterStemmer()

def porter_stemming(dataset):
    processed_dataset = []
    for i, tweet in enumerate(dataset):
        tweet_tokens = word_tokenize(tweet)
        stemmed_tweet_tokens = [porter.stem(w) for w in tweet_tokens]
        processed_dataset.append(' '.join(stemmed_tweet_tokens))
    return processed_dataset

def preprocess(dataset):
  dataset = pd.Series(remove_nonascii(dataset))
  dataset = pd.Series(remove_hashtags(dataset))
  dataset = pd.Series(remove_hyperlinks(dataset))
  dataset = pd.Series(remove_stopwords(dataset))
  dataset = pd.Series(remove_punctuations(dataset))
  dataset = pd.Series(porter_stemming(dataset))
  return dataset

if __name__ == '__main__':
  all_positive_tweets = twitter_samples.strings('positive_tweets.json')
  all_negative_tweets = twitter_samples.strings('negative_tweets.json')

  positive_dict = pd.DataFrame({'tweet': all_positive_tweets, 'sentiment': 'positive'})
  negative_dict = pd.DataFrame({'tweet': all_negative_tweets, 'sentiment': 'negative'})
  tweets = positive_dict.append(negative_dict, ignore_index = True)

  from sklearn.model_selection import train_test_split
  tweets = preprocess(tweets)
  
  tweets.to_gbq(destination_table = 'twitterstream', project_id = 'bigdata6893-328419', if_exists = 'append')
  
